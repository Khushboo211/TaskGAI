{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07110d6d-3e74-46b1-a1ae-56a6c8628892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sagar\\anaconda3\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    }
   ],
   "source": [
    "#All modules and Libraries Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f7ad3b2-f538-4545-a36a-998cc785bfb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: []\n"
     ]
    }
   ],
   "source": [
    "#GPU Setup\n",
    "print(\"Available GPUs:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15654bbb-f85d-4a5d-9d93-4a054e214fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 6285444\n"
     ]
    }
   ],
   "source": [
    "#Load MULTIPLE .txt Files\n",
    "DATASET_PATH = r\"C:\\Users\\sagar\\Documents\\New folder (2)\"\n",
    "\n",
    "def load_text(folder):\n",
    "    texts = []\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".txt\"):\n",
    "            with open(os.path.join(folder, file),\n",
    "                      encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                texts.append(f.read().lower())\n",
    "    return \"\\n\".join(texts)\n",
    "\n",
    "text = load_text(DATASET_PATH)\n",
    "print(\"Total characters:\", len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f51362-a9fa-4c74-890d-64f1e87fcd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character Tokenization\n",
    "chars = sorted(set(text))\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char_to_idx = {c: i for i, c in enumerate(chars)}\n",
    "idx_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "encoded_text = np.array([char_to_idx[c] for c in text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32730262-2c4d-4583-8f2b-a2ced421cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Memory-Safe Data Generator\n",
    "SEQ_LENGTH = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def data_generator(encoded, seq_len, batch_size):\n",
    "    while True:\n",
    "        X, y = [], []\n",
    "        for _ in range(batch_size):\n",
    "            idx = np.random.randint(0, len(encoded) - seq_len - 1)\n",
    "            X.append(encoded[idx:idx + seq_len])\n",
    "            y.append(encoded[idx + seq_len])\n",
    "        yield np.array(X), np.array(y)\n",
    "\n",
    "steps_per_epoch = len(encoded_text) // (SEQ_LENGTH * BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bcbc033-c137-4e4d-a694-129da5a80c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Optimized LSTM Model\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 128),\n",
    "    LSTM(256, return_sequences=True),\n",
    "    Dropout(0.2),\n",
    "    LSTM(128),\n",
    "    Dense(vocab_size, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\"\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6460b2-7fc4-4d58-b2a6-9e9b46048549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "\u001b[1m1964/1964\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m644s\u001b[0m 328ms/step - loss: 2.3321\n",
      "Epoch 2/2\n",
      "\u001b[1m1964/1964\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m667s\u001b[0m 340ms/step - loss: 2.0048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x210d0747610>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"loss\", patience=3),\n",
    "    ModelCheckpoint(\n",
    "        \"lstm_text_generator.keras\",\n",
    "        monitor=\"loss\",\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    data_generator(encoded_text, SEQ_LENGTH, BATCH_SIZE),\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=2,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2481d95e-2c85-463f-bd8e-af72e0c88cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temperature Sampling\n",
    "def sample_with_temperature(preds, temperature=0.8):\n",
    "    preds = np.log(preds + 1e-8) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    return np.random.choice(len(preds), p=preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71ff0d59-584e-4f33-b4db-203d652c0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Text OUTPUT\n",
    "def generate_text(seed_text, length=300, temperature=0.8):\n",
    "    seed_text = seed_text.lower()\n",
    "    result = seed_text\n",
    "\n",
    "    for _ in range(length):\n",
    "        encoded = [char_to_idx.get(c, 0) for c in seed_text]\n",
    "        encoded = pad_sequences([encoded], maxlen=SEQ_LENGTH, truncating=\"pre\")\n",
    "\n",
    "        preds = model.predict(encoded, verbose=0)[0]\n",
    "        next_idx = sample_with_temperature(preds, temperature)\n",
    "        next_char = idx_to_char[next_idx]\n",
    "\n",
    "        result += next_char\n",
    "        seed_text += next_char\n",
    "        seed_text = seed_text[1:]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ae83e31-ce2f-4493-89c2-6425d8b774a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "to be or not to benenting about at babmon in the his lidaud moon the harry masted out the with the loo cout his you rest apout to ham lith muin!’\n",
      "\n",
      "“momed,” snormon was wall said acle iom a gow a shisce in the corceately dook, and weren. whour that he waw mastly a hay witklil of bear of mutigald of the mele, brace to the wand gotter reemy of to con fourd the said of had and that wempooged to who grape bewed bething \n"
     ]
    }
   ],
   "source": [
    "#OUTPUT\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "print(generate_text(\"to be or not to be\", length=400, temperature=0.7))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
